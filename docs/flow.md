# 系统流程说明（基于大语言模型的文本处理辅助系统）

本系统以“文本输入 → 大语言模型处理 → 结构化输出 → 结果评估”为主线，构建了一套可运行、可评估、可扩展的文本处理辅助系统。整体流程如下。

---

## 1. 输入阶段（Input）
系统从本地读取用户提供的文本文件（`.txt` 格式），作为待处理的原始文本输入。  
该方式便于系统作为命令行工具使用，也为后续批量处理和评估提供基础。

---

## 2. 提示词模板加载（Prompt Template）
系统从 `prompts/extract_all.json.md` 中加载预先设计好的提示词模板。  
该模板明确规定了模型输出的 JSON 结构与字段约束，并通过占位符将输入文本动态填充到提示词中，实现提示词与业务逻辑的解耦，便于后续迭代优化。

---

## 3. 大语言模型推理（LLM Inference）
系统调用 OpenAI 的大语言模型接口，对文本进行一次性推理处理。  
在单次调用中同时完成摘要生成、要点提取、主题分类、情绪判断、关键词抽取、实体识别以及正式改写等任务，从而减少接口调用次数，提高系统稳定性与效率。

---

## 4. 输出校验与解析（Check Outputs）
模型返回结果后，系统首先对输出内容进行格式检查与解析处理。  
通过自定义的安全解析函数，自动去除可能出现的代码块标记或多余文本，并从中提取合法的 JSON 数据；同时对缺失字段进行默认补全，确保输出结构始终完整可用，避免因格式问题导致程序中断。

---

## 5. 结果输出（Outputs）
在解析完成后，系统生成两类输出文件：
- **结构化结果文件（`.result.json`）**：用于程序处理、评估与后续系统集成；
- **可读报告文件（`.report.md`）**：以 Markdown 形式展示处理结果，便于人工阅读、汇报与展示。

---

## 6. 系统评估（Evaluation）
系统通过 `eval.py` 脚本对预设测试集进行自动化评估。  
评估指标包括 JSON 输出是否合规、主题分类是否命中、情绪判断是否正确等，并将评估结果记录在文档中，以支持系统效果验证与后续迭代优化。

---

## 7. 流程总结
通过上述流程，系统实现了从文本输入到结构化输出、再到量化评估的完整闭环，体现了大语言模型在文本理解与处理任务中的工程化应用能力。